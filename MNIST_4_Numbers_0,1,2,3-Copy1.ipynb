{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32 #大概需要2G的显存\n",
    "EPOCHS=5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_top(x):\n",
    "    nsam = x.shape[0]\n",
    "    nrow = x.shape[1]\n",
    "    ncol = x.shape[2]\n",
    "    z = np.zeros((nsam, nrow, ncol), dtype=np.double)\n",
    "    data1 = np.concatenate((x, z), axis=1)\n",
    "    data2 = np.concatenate((z, z), axis=1)\n",
    "    data3 = np.concatenate((data1, data2), 2)\n",
    "    return data3\n",
    "def right_bottom(x):\n",
    "    nsam = x.shape[0]\n",
    "    nrow = x.shape[1]\n",
    "    ncol = x.shape[2]\n",
    "    z = np.zeros((nsam, nrow, ncol), dtype=np.double)\n",
    "    data1 = np.concatenate((z, z), axis=1)\n",
    "    data2 = np.concatenate((z, x), axis=1)\n",
    "    data3 = np.concatenate((data1, data2), 2)\n",
    "    return data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getType(dataset,x):\n",
    "    index = np.where(dataset[:, 0] == x)\n",
    "    for i in index:\n",
    "        Type = dataset[i]\n",
    "    return Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPixel(x):\n",
    "    pixel = x[:, 1:]\n",
    "    return pixel\n",
    "def getLabel(x):\n",
    "    label = x[:, 0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        mnist_train = pd.read_csv('mnist_train.csv')\n",
    "        train = mnist_train.values\n",
    "\n",
    "        mnist_train_0=getType(train,0)\n",
    "        mnist_train_1=getType(train,1)   \n",
    "        mnist_train_2=getType(train,2)\n",
    "        mnist_train_3=getType(train,3)   \n",
    "        \n",
    "            \n",
    "        nrow = 28\n",
    "        ncol = 28\n",
    "\n",
    "        # X and label\n",
    "        x1 = getPixel(mnist_train_1)\n",
    "        x0 =getPixel(mnist_train_0)\n",
    "        y1 = getLabel(mnist_train_1)\n",
    "        y0 = getLabel(mnist_train_0)\n",
    "        x2 = getPixel(mnist_train_2)\n",
    "        x3 =getPixel(mnist_train_3)\n",
    "        y2= getLabel(mnist_train_2)\n",
    "        y3 = getLabel(mnist_train_3)\n",
    "\n",
    "        # X reshape to 28 28\n",
    "               \n",
    "        nx1 = x1.shape[0]\n",
    "        nx0 = x0.shape[0]\n",
    "        nx2 = x2.shape[0]\n",
    "        nx3 = x3.shape[0]\n",
    "       \n",
    "        x0_reshape = x0.reshape((nx0, nrow, ncol))\n",
    "        x1_reshape = x1.reshape((nx1, nrow, ncol))\n",
    "        x2_reshape = x2.reshape((nx2, nrow, ncol))\n",
    "        x3_reshape = x3.reshape((nx3, nrow, ncol))\n",
    "\n",
    "\n",
    "        left0 = left_top(x0_reshape)\n",
    "        right1 = right_bottom(x1_reshape)\n",
    "        \n",
    "        left2=left_top(x2_reshape)\n",
    "        right3=right_bottom(x3_reshape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_data1 = np.concatenate((left0, right1), axis=0)\n",
    "        train_data2 = np.concatenate((left2, right3), axis=0)\n",
    "        train_data= np.concatenate((train_data1, train_data2), axis=0)\n",
    "        \n",
    "        nn = train_data.shape[0]\n",
    "\n",
    "        # reshape to 3136 which is 56*56\n",
    "        train_data_reshape = train_data\n",
    "        train_data_reshape = torch.from_numpy(train_data_reshape)\n",
    "        train_data_reshape = torch.unsqueeze(train_data_reshape, 1)\n",
    "        \n",
    "        yy1 = np.concatenate((y0, y1), axis=0)\n",
    "        yy2=np.concatenate((y2, y3), axis=0)\n",
    "        yy=np.concatenate((yy1, yy2), axis=0)\n",
    "        \n",
    "        yy=torch.from_numpy(yy)\n",
    "      \n",
    "        self.len = yy.shape[0]\n",
    "\n",
    "        self.x_data = train_data_reshape\n",
    "\n",
    "        self.y_data = yy\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "dataset1= train()\n",
    "train_loader = DataLoader(dataset=dataset1,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(Dataset):\n",
    "    def __init__(self):\n",
    "  \n",
    "        mnist_test = pd.read_csv('mnist_test.csv')\n",
    "        testtest= mnist_test.values\n",
    "        \n",
    "        \n",
    "        test0=getType(testtest,0)\n",
    "        test1=getType(testtest,1)\n",
    "        test2=getType(testtest,2)\n",
    "        test3=getType(testtest,3)\n",
    "        nrow = 28\n",
    "        ncol = 28\n",
    "\n",
    "        # X and label\n",
    "        x1 = getPixel(test1)\n",
    "        x0 =getPixel(test0)\n",
    "        y1 = getLabel(test1)\n",
    "        y0 = getLabel(test0)\n",
    "        x2 = getPixel(test2)\n",
    "        x3 =getPixel(test3)\n",
    "        y2 = getLabel(test2)\n",
    "        y3 = getLabel(test3)\n",
    "\n",
    "\n",
    "\n",
    "        # X reshape to 28 28\n",
    "        nx1 = x1.shape[0]\n",
    "        nx0 = x0.shape[0]\n",
    "        nx2 = x2.shape[0]\n",
    "        nx3 = x3.shape[0]\n",
    "      \n",
    "        x0_reshape = x0.reshape((nx0, nrow, ncol))\n",
    "        x1_reshape = x1.reshape((nx1, nrow, ncol))\n",
    "        x2_reshape = x2.reshape((nx2, nrow, ncol))\n",
    "        x3_reshape = x3.reshape((nx3, nrow, ncol))\n",
    "\n",
    "\n",
    "        left0 = left_top(x0_reshape)\n",
    "        right1 = right_bottom(x1_reshape)\n",
    "        \n",
    "        left2=left_top(x2_reshape)\n",
    "        right3=right_bottom(x3_reshape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_data1 = np.concatenate((left0, right1), axis=0)\n",
    "        train_data2 = np.concatenate((left2, right3), axis=0)\n",
    "        train_data= np.concatenate((train_data1, train_data2), axis=0)\n",
    "\n",
    "        \n",
    "\n",
    "        nn = train_data.shape[0]\n",
    "\n",
    "        \n",
    "        train_data_reshape = train_data\n",
    "        train_data_reshape = torch.from_numpy(train_data_reshape)\n",
    "        train_data_reshape = torch.unsqueeze(train_data_reshape, 1)\n",
    "        \n",
    "        yy1 = np.concatenate((y0, y1), axis=0)\n",
    "        yy2=np.concatenate((y2, y3), axis=0)\n",
    "        yy=np.concatenate((yy1, yy2), axis=0)\n",
    "        \n",
    "        yy=torch.from_numpy(yy)\n",
    "      \n",
    "        self.len = yy.shape[0]\n",
    "\n",
    "        self.x_data = train_data_reshape\n",
    "\n",
    "        self.y_data = yy\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # img = Image.fromarray(self.x_data[index])\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "dataset2 = test()\n",
    "test_loader = DataLoader(dataset=dataset2,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.conv1=nn.Conv2d(1,10,5) # 10, 24x24\n",
    "        self.conv2=nn.Conv2d(10,20,3) # 128, 10x10\n",
    "        self.fc1 = nn.Linear(2880,300)\n",
    "        self.fc2 = nn.Linear(300,4)\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x) #24\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)  #12\n",
    "        out = self.conv2(out) #10\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)  #12\n",
    "        out = F.relu(out)   \n",
    "        out = out.view(in_size,-1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = F.log_softmax(out,dim=1)\n",
    "        # out = F.softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model = model.to(DEVICE)\n",
    "model= model.double()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # print(data.size(), target.size())\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        # print(output)\n",
    "        #print(target)\n",
    "        # loss = F.nll_loss(output, target)\n",
    "        # print(loss)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data.double())\n",
    "#             print(output)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1568/24754 (6%)]\tLoss: 0.004852\n",
      "Train Epoch: 1 [3168/24754 (13%)]\tLoss: 0.009871\n",
      "Train Epoch: 1 [4768/24754 (19%)]\tLoss: 0.000733\n",
      "Train Epoch: 1 [6368/24754 (26%)]\tLoss: 0.048986\n",
      "Train Epoch: 1 [7968/24754 (32%)]\tLoss: 0.000144\n",
      "Train Epoch: 1 [9568/24754 (39%)]\tLoss: 0.010032\n",
      "Train Epoch: 1 [11168/24754 (45%)]\tLoss: 0.001551\n",
      "Train Epoch: 1 [12768/24754 (52%)]\tLoss: 0.000026\n",
      "Train Epoch: 1 [14368/24754 (58%)]\tLoss: 0.001063\n",
      "Train Epoch: 1 [15968/24754 (64%)]\tLoss: 0.000072\n",
      "Train Epoch: 1 [17568/24754 (71%)]\tLoss: 0.000702\n",
      "Train Epoch: 1 [19168/24754 (77%)]\tLoss: 0.000133\n",
      "Train Epoch: 1 [20768/24754 (84%)]\tLoss: 0.001155\n",
      "Train Epoch: 1 [22368/24754 (90%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [23968/24754 (97%)]\tLoss: 0.015359\n",
      "\n",
      "Test set: Average loss: 0.0053, Accuracy: 4147/4157 (99.7594%)\n",
      "\n",
      "Train Epoch: 2 [1568/24754 (6%)]\tLoss: 0.000136\n",
      "Train Epoch: 2 [3168/24754 (13%)]\tLoss: 0.004158\n",
      "Train Epoch: 2 [4768/24754 (19%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [6368/24754 (26%)]\tLoss: 0.000010\n",
      "Train Epoch: 2 [7968/24754 (32%)]\tLoss: 0.184429\n",
      "Train Epoch: 2 [9568/24754 (39%)]\tLoss: 0.001087\n",
      "Train Epoch: 2 [11168/24754 (45%)]\tLoss: 0.021856\n",
      "Train Epoch: 2 [12768/24754 (52%)]\tLoss: 0.000011\n",
      "Train Epoch: 2 [14368/24754 (58%)]\tLoss: 0.001334\n",
      "Train Epoch: 2 [15968/24754 (64%)]\tLoss: 0.028964\n",
      "Train Epoch: 2 [17568/24754 (71%)]\tLoss: 0.002904\n",
      "Train Epoch: 2 [19168/24754 (77%)]\tLoss: 0.000107\n",
      "Train Epoch: 2 [20768/24754 (84%)]\tLoss: 0.004076\n",
      "Train Epoch: 2 [22368/24754 (90%)]\tLoss: 0.000700\n",
      "Train Epoch: 2 [23968/24754 (97%)]\tLoss: 0.000028\n",
      "\n",
      "Test set: Average loss: 0.0049, Accuracy: 4150/4157 (99.8316%)\n",
      "\n",
      "Train Epoch: 3 [1568/24754 (6%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [3168/24754 (13%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [4768/24754 (19%)]\tLoss: 0.000065\n",
      "Train Epoch: 3 [6368/24754 (26%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7968/24754 (32%)]\tLoss: 0.000005\n",
      "Train Epoch: 3 [9568/24754 (39%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [11168/24754 (45%)]\tLoss: 0.000170\n",
      "Train Epoch: 3 [12768/24754 (52%)]\tLoss: 0.000074\n",
      "Train Epoch: 3 [14368/24754 (58%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [15968/24754 (64%)]\tLoss: 0.000003\n",
      "Train Epoch: 3 [17568/24754 (71%)]\tLoss: 0.000274\n",
      "Train Epoch: 3 [19168/24754 (77%)]\tLoss: 0.017310\n",
      "Train Epoch: 3 [20768/24754 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [22368/24754 (90%)]\tLoss: 0.000038\n",
      "Train Epoch: 3 [23968/24754 (97%)]\tLoss: 0.000009\n",
      "\n",
      "Test set: Average loss: 0.0244, Accuracy: 4128/4157 (99.3024%)\n",
      "\n",
      "Train Epoch: 4 [1568/24754 (6%)]\tLoss: 0.034699\n",
      "Train Epoch: 4 [3168/24754 (13%)]\tLoss: 0.000023\n",
      "Train Epoch: 4 [4768/24754 (19%)]\tLoss: 0.063875\n",
      "Train Epoch: 4 [6368/24754 (26%)]\tLoss: 0.000001\n",
      "Train Epoch: 4 [7968/24754 (32%)]\tLoss: 0.000150\n",
      "Train Epoch: 4 [9568/24754 (39%)]\tLoss: 0.000036\n",
      "Train Epoch: 4 [11168/24754 (45%)]\tLoss: 0.000829\n",
      "Train Epoch: 4 [12768/24754 (52%)]\tLoss: 0.021323\n",
      "Train Epoch: 4 [14368/24754 (58%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [15968/24754 (64%)]\tLoss: 0.000506\n",
      "Train Epoch: 4 [17568/24754 (71%)]\tLoss: 0.000020\n",
      "Train Epoch: 4 [19168/24754 (77%)]\tLoss: 0.288183\n",
      "Train Epoch: 4 [20768/24754 (84%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [22368/24754 (90%)]\tLoss: 0.212830\n",
      "Train Epoch: 4 [23968/24754 (97%)]\tLoss: 0.000200\n",
      "\n",
      "Test set: Average loss: 0.0072, Accuracy: 4145/4157 (99.7113%)\n",
      "\n",
      "Train Epoch: 5 [1568/24754 (6%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [3168/24754 (13%)]\tLoss: 0.000348\n",
      "Train Epoch: 5 [4768/24754 (19%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [6368/24754 (26%)]\tLoss: 0.000014\n",
      "Train Epoch: 5 [7968/24754 (32%)]\tLoss: 0.000425\n",
      "Train Epoch: 5 [9568/24754 (39%)]\tLoss: 0.010303\n",
      "Train Epoch: 5 [11168/24754 (45%)]\tLoss: 0.396022\n",
      "Train Epoch: 5 [12768/24754 (52%)]\tLoss: 0.001147\n",
      "Train Epoch: 5 [14368/24754 (58%)]\tLoss: 0.000012\n",
      "Train Epoch: 5 [15968/24754 (64%)]\tLoss: 0.000014\n",
      "Train Epoch: 5 [17568/24754 (71%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [19168/24754 (77%)]\tLoss: 0.000022\n",
      "Train Epoch: 5 [20768/24754 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [22368/24754 (90%)]\tLoss: 0.000041\n",
      "Train Epoch: 5 [23968/24754 (97%)]\tLoss: 0.000052\n",
      "\n",
      "Test set: Average loss: 0.0053, Accuracy: 4150/4157 (99.8316%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
